{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reverse_DictionaryV1",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOEpnQMXOVL9aIOL1qBPETr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dataSci-rigo/Reverse_Dictionary/blob/master/Reverse_DictionaryV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-Ob04T8pE37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install -q kaggle\n",
        "kaggle = {\"username\":\"rodrodington\",\"key\":\"88dbb428493d4bd041c0072c69e28793\"}\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/\" # put path for wherever you put it\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle['username'] # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = kaggle['key'] #key from the json file\n",
        "! kaggle datasets download -d therohk/urban-dictionary-words-dataset/version/2\n",
        "!unzip -q urban-dictionary-words-dataset.zip -d .\n",
        "import pandas as pd\n",
        "\n",
        "urban_raw_file = '/content/urbandict-word-defs.csv'\n",
        "urban_dict = pd.read_csv(urban_raw_file,error_bad_lines=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvzkeECjpmg0",
        "colab_type": "code",
        "outputId": "2ddfdbdc-7a27-4f65-a221-a4e90498235f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "urban_dict['net_votes'] = urban_dict[\"up_votes\"]-urban_dict[\"down_votes\"]\n",
        "dict_size = urban_dict.shape[0]\n",
        "print(\"number of entries\",dict_size)\n",
        "urban_dict=urban_dict[(urban_dict.word !=None) & (urban_dict.net_votes>100) & (urban_dict.word.str.contains('[A-Za-z]'))].sort_values('word')\n",
        "print(\"number of entries\",urban_dict.shape)\n",
        "urban_dict.head(10)\n",
        "print(urban_dict.word.unique().shape,urban_dict.shape, urban_dict.dropna().shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of entries 2580586\n",
            "number of entries (129647, 7)\n",
            "(79513,) (129647, 7) (129629, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcJsjVel6xXM",
        "colab_type": "code",
        "outputId": "0273cf64-b87a-4fb1-84ca-2bea3df8812d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "!wget -O dictionary.csv https://www.bragitoff.com/wp-content/uploads/2016/03/dictionary.csv\n",
        "\n",
        "dictionary= pd.read_csv('/content/dictionary.csv', header=None)\n",
        "dictionary.head(100)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-04 03:17:59--  https://www.bragitoff.com/wp-content/uploads/2016/03/dictionary.csv\n",
            "Resolving www.bragitoff.com (www.bragitoff.com)... 104.28.7.107, 104.28.6.107, 2606:4700:3037::681c:76b, ...\n",
            "Connecting to www.bragitoff.com (www.bragitoff.com)|104.28.7.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘dictionary.csv’\n",
            "\n",
            "\rdictionary.csv          [<=>                 ]       0  --.-KB/s               \rdictionary.csv          [ <=>                ]   4.51M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-05-04 03:17:59 (48.1 MB/s) - ‘dictionary.csv’ saved [4725252]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The first letter of the English and of many ot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The name of the sixth tone in the model major ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>An adjective, commonly called the indefinite a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In each; to or for each; as, \"twenty leagues a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A</td>\n",
              "      <td>prep.</td>\n",
              "      <td>In; on; at; by.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Abassis</td>\n",
              "      <td>n.</td>\n",
              "      <td>A silver coin of Persia, worth about twenty ce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Abatable</td>\n",
              "      <td>a.</td>\n",
              "      <td>Capable of being abated; as, an abatable writ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Abated</td>\n",
              "      <td>imp. &amp; p. p.</td>\n",
              "      <td>of Abate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Abating</td>\n",
              "      <td>p. pr. &amp; vb. n.</td>\n",
              "      <td>of Abate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Abate</td>\n",
              "      <td>v. t.</td>\n",
              "      <td>To beat down; to overthrow.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0  ...                                                  2\n",
              "0          A  ...  The first letter of the English and of many ot...\n",
              "1          A  ...  The name of the sixth tone in the model major ...\n",
              "2          A  ...  An adjective, commonly called the indefinite a...\n",
              "3          A  ...  In each; to or for each; as, \"twenty leagues a...\n",
              "4          A  ...                                    In; on; at; by.\n",
              "..       ...  ...                                                ...\n",
              "95   Abassis  ...  A silver coin of Persia, worth about twenty ce...\n",
              "96  Abatable  ...  Capable of being abated; as, an abatable writ ...\n",
              "97    Abated  ...                                           of Abate\n",
              "98   Abating  ...                                           of Abate\n",
              "99     Abate  ...                        To beat down; to overthrow.\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcwUxB8S704Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/UKPLab/sentence-transformers\n",
        "%cd sentence-transformers\n",
        "!python -m pip install -e .\n",
        "from sentence_transformers import SentenceTransformer\n",
        "%cd ..\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExjGtn7wGPrb",
        "colab_type": "code",
        "outputId": "dd5cf2cf-b4f0-4650-de04-dadba9b9cb5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.31G/1.31G [00:47<00:00, 27.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc8M7RbopOYy",
        "colab_type": "code",
        "outputId": "f3dc4da4-81e6-458b-d8b2-80b81333bce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dictionary_definitions = dictionary[2].tolist()\n",
        "dictionary_words = dictionary[0].tolist()\n",
        "dictionary_tense= dictionary[1].tolist()\n",
        "urban_dict=urban_dict.dropna()\n",
        "urban_words=urban_dict['word'].tolist()\n",
        "urban_definitions=urban_dict['definition'].tolist()\n",
        "print(len(dictionary_words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrULH8fU3Zas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary_embeddings = model.encode(dictionary_definitions)\n",
        "urban_embeddings = model.encode(urban_definitions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXe8cENg5L3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import boto3\n",
        "s3r_write = boto3.resource('s3', aws_access_key_id='AKIA32S7UTDXS2OX5FMX',\n",
        "    aws_secret_access_key='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')\n",
        "write_buck = s3r_write.Bucket('humorbot3000results')\n",
        "np.save('urban.npy',np.array([urban_definitions,urban_words,urban_embeddings]))\n",
        "write_buck.upload_file('urban.npy','urban.npy')\n",
        "np.save('dictionary.npy',np.array([dictionary_definitions,dictionary_words,dictionary_embeddings,dictionary_tense]))\n",
        "write_buck.upload_file('dictionary.npy','dictionary.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnVVVkj-pFvF",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "9426ad6e-42e6-4f30-ebc7-07f5d2f2ad36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "query = 'a smart person' #@param {type: 'string'}\n",
        "\n",
        "queries = [query]\n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\n",
        "number_top_matches =  5#@param {type: \"number\"}\n",
        "\n",
        "print(\"Semantic Search Results\")\n",
        "\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    urban_distances = scipy.spatial.distance.cdist([query_embedding], urban_embeddings, \"cosine\")[0]\n",
        "    dictionary_distances = scipy.spatial.distance.cdist([query_embedding], dictionary_embeddings, \"cosine\")[0]\n",
        "    urban_results = zip(range(len(urban_distances)), urban_distances)\n",
        "    urban_results = sorted(urban_results, key=lambda x: x[1])\n",
        "    dictionary_results = zip(range(len(dictionary_distances)), dictionary_distances)\n",
        "    dictionary_results = sorted(dictionary_results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar Standard English words:\")\n",
        "\n",
        "    for idx, dictionary_distance in  dictionary_results[0:number_top_matches]:\n",
        "        print(dictionary_words[idx],\"#\",dictionary_tense[idx],dictionary_definitions[idx].strip(), \"(Cosine Score: %.4f)\" % (1-dictionary_distance))\n",
        "    print(\"\\nTop 5 most similar Urban Dictionary words:\")\n",
        "    for idx, urban_distance in urban_results[0:number_top_matches]:\n",
        "        print(urban_words[idx],\"#\",urban_definitions[idx].strip(), \"(Cosine Score: %.4f)\" % (1-urban_distance))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Semantic Search Results\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: a smart person\n",
            "\n",
            "Top 5 most similar Standard English words:\n",
            "Hende # a. Skillful; dexterous; clever. (Cosine Score: 0.7807)\n",
            "Scient # a. Knowing; skillful. (Cosine Score: 0.7803)\n",
            "Adept # a. Well skilled; completely versed; thoroughly proficient. (Cosine Score: 0.7505)\n",
            "Renownful # a. Having great renown; famous. (Cosine Score: 0.7416)\n",
            "Understanding # a. Knowing; intelligent; skillful; as, he is an understanding man. (Cosine Score: 0.7409)\n",
            "\n",
            "Top 5 most similar Urban Dictionary words:\n",
            "zaid # an awesome guy who is smart (Cosine Score: 0.9283)\n",
            "Ren # Very talented person. (Cosine Score: 0.9100)\n",
            "Naveed # Beautiful person, smart and intelligent (Cosine Score: 0.8988)\n",
            "Aniyah # A good friend who is quite smart. (Cosine Score: 0.8959)\n",
            "Rene # the most amazing person ever; usually smart (Cosine Score: 0.8902)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRItCZuUdGP8",
        "colab_type": "text"
      },
      "source": [
        "EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y5nKXlyr9d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line     = []\n",
        "expected = []\n",
        "saw      = []     \n",
        "cont     = True \n",
        "urban_raw_file = '/content/urbandict-word-defs.csv'\n",
        "while cont == True:     \n",
        "    try:\n",
        "        data = pd.read_csv(urban_raw_file,skiprows=line)\n",
        "        cont = False\n",
        "    except Exception as e:    \n",
        "        # message = property(_get_message,\n",
        "        #                 doc=\"access the 'message' attribute; \"\n",
        "        #                     \"deprecated and provided only for \"\n",
        "        #                     \"backwards-compatibility\")\n",
        "        errortype = str(e).split('.')[0].strip()                                \n",
        "        if errortype == 'Error tokenizing data':                        \n",
        "           cerror      = str(e).split(':')[1].strip().replace(',','')\n",
        "           nums        = [n for n in cerror.split(' ') if str.isdigit(n)]\n",
        "           expected.append(int(nums[0]))\n",
        "           saw.append(int(nums[2]))\n",
        "           line.append(int(nums[1])-1)\n",
        "        else:\n",
        "           cerror      = 'Unknown'\n",
        "           print ('Unknown Error - 222')\n",
        "\n",
        "len(line)\n",
        "with open(urban_raw_file) as f:\n",
        "  print(f.readline())\n",
        "  dict_size=0\n",
        "  skipped_words=0\n",
        "  for line in f.readlines():\n",
        "    split_line = line.strip().split('\"')\n",
        "    skipped_words+=1\n",
        "    if len(split_line[0].split(\",\"))!=6:\n",
        "      count2+=1\n",
        "      first_half =split_line[0].split(\",\")[:-1]\n",
        "      idx=first_half[0]\n",
        "      up_votes = first_half[-3]\n",
        "      down_votes = first_half[-2]\n",
        "      excess_len=len(first_half)-5\n",
        "      if  int(up_votes)>30:\n",
        "        print(idx,up_votes,down_votes,excess_len, '|'.join(first_half[1:excess_len+2]))\n",
        "        print(line)\n",
        "        #print(split_line,split_line[0].split(\",\") )\n",
        "  print('dictionary size and skipped words',dict_size,skipped_words)\n",
        "\n",
        "print(data['word_id'].dtype,\n",
        "data['down_votes'].dtype,\n",
        "data['up_votes'].dtype,)\n",
        "print(data['down_votes'].max(),\n",
        "data['down_votes'].mean(), data['down_votes'].median(),\n",
        "data['up_votes'].max(),\n",
        "data['up_votes'].mean(),\n",
        "data['up_votes'].median())\n",
        "data['net_votes'] = data[\"up_votes\"]-data[\"down_votes\"]\n",
        "print(data['net_votes'].max(),\n",
        "data['net_votes'].mean(),\n",
        "data['net_votes'].median())\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}